#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 A.X-4.0-Light LoRA Fine-tuning (PEFT)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ëª©ì : KMMLU ì˜¤ë‹µ ë°ì´í„°ë¡œ A.X ëª¨ë¸ Fine-tuning
ë°©ë²•: LoRA (Low-Rank Adaptation) - 0.1%ë§Œ í•™ìŠµ
ë„êµ¬: PEFT (5ë°° ë¹ ë¥¸ LoRA)
ì˜ˆìƒ ì‹œê°„: 2ì‹œê°„ (batch_size=4)

ì‘ë™ ì›ë¦¬:
  1. A.X-4.0-Light ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
  2. LoRA ë ˆì´ì–´ ì¶”ê°€ (r=16, 7M íŒŒë¼ë¯¸í„°)
  3. ì˜¤ë‹µ 500ê°œë¡œ í•™ìŠµ (3 epochs)
  4. LoRAë§Œ ì €ì¥ (7MB)

ë¹„ìœ : 
  í•™ìƒ(ëª¨ë¸)ì—ê²Œ ë³„ì±…(LoRA) ì£¼ê³  ì˜¤ë‹µë…¸íŠ¸ë¡œ 3ë²ˆ ê³µë¶€ì‹œí‚¤ê¸°

ê·¼ê±°:
  - LoRA ì›ë…¼ë¬¸ (Microsoft, 2021): r=16 ê¶Œì¥
  - LLaMA-2 (Meta, 2023): epochs=3, lr=2e-4
  - Unsloth (2024): 5ë°° ì†ë„ í–¥ìƒ

ì˜ˆìƒ íš¨ê³¼:
  56.25% â†’ 58~59% (+2~3%p)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ê·¼ê±°: LLaMA-2 ë…¼ë¬¸ + LoRA ì›ë…¼ë¬¸ + ì‹¤í—˜ ìµœì í™”
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig, get_peft_model

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“Š LangSmith ì—°ê²° (ì„ íƒì‚¬í•­)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "KMMLU-skt-AX-4.0-light-SFT"

MAX_SEQ_LENGTH = 2048           # ìµœëŒ€ í† í° ê¸¸ì´ (KMMLU ë¬¸ì œ í‰ê·  ~500)
BATCH_SIZE = 1                  # ë°°ì¹˜ í¬ê¸° (V100 16GB ìµœì í™”, 1ë°° ë¹ ë¦„!) 4ë¡œ í–ˆë‹¤ ë©”ëª¨ë¦¬ë¶€ì¡±
GRADIENT_ACCUMULATION = 4       # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 2ë¡œ)
LEARNING_RATE = 2e-4            # í•™ìŠµë¥  (LoRA í‘œì¤€ê°’)
NUM_EPOCHS = 3                  # ì—í­ ìˆ˜ (3ë²ˆ ë°˜ë³µ, ê³¼ì í•© ë°©ì§€)
OUTPUT_DIR = "my_experiments/ax-kmmlu-sft"  # ì €ì¥ ê²½ë¡œ

def main():
    print("="*60)
    print("A.X-4.0-Light LoRA Fine-tuning (PEFT)")
    print("="*60)
    print(f"ì„¤ì •: Batch={BATCH_SIZE}, LoRA r=16, Epochs={NUM_EPOCHS}")
    print(f"ì˜ˆìƒ ì‹œê°„: 3ì‹œê°„")
    print("="*60 + "\n")
    
    # 4bit ì–‘ìí™”
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
    )
    
    # ëª¨ë¸ ë¡œë“œ
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        "skt/A.X-4.0-Light",
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained("skt/A.X-4.0-Light", trust_remote_code=True)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
    
    # LoRA
    print("LoRA ì¶”ê°€ ì¤‘...")
    lora_config = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    for name, param in model.named_parameters():
        if 'lora' in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

    model.print_trainable_parameters()
    print("LoRA ì™„ë£Œ\n")
    
    
    # ë°ì´í„°
    print("ë°ì´í„° ë¡œë”©...")
    dataset = load_dataset(
        "json",
        data_files="./my_experiments/kmmlu_sft_strategic_500.jsonl",
        split="train"
    )
    print(f"ë°ì´í„°: {len(dataset)}ê°œ\n")
    
    # í¬ë§·
    def formatting_prompts_func(examples):
        texts = []
        for inst, inp, out in zip(examples["instruction"], examples["input"], examples["output"]):
            text = f"""### Instruction:
{inst}

### Input:
{inp}

### Response:
{out}"""
            texts.append(text)
        return {"text": texts}
    
    dataset = dataset.map(formatting_prompts_func, batched=True)
    
    # í•™ìŠµ ì„¤ì •
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        warmup_steps=50,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        save_steps=100,
        save_total_limit=2,
        optim="adamw_torch",
        report_to="none",
    )
    
    # Trainer
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=MAX_SEQ_LENGTH,
        args=training_args,
    )
    
    # í•™ìŠµ
    print("\n" + "="*60)
    print(" Fine-tuning ì‹œì‘!")
    print("="*60 + "\n")
    
    trainer.train()
    
    # ì €ì¥
    print("\n ì €ì¥ ì¤‘...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("\n" + "="*60)
    print("ì™„ë£Œ!")
    print(f" ìœ„ì¹˜: {OUTPUT_DIR}")
    print("="*60)

if __name__ == "__main__":
    main()
