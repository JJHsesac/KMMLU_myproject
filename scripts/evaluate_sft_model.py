#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” SFT ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ëª©ì : Fine-tuned LoRA ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
ë¹„êµ: Zero-shot vs SFT ê²°ê³¼
ë°ì´í„°: KMMLU ì „ì²´ 45ê°œ ê³¼ëª©

ì¶œë ¥:
1. JSON: ìƒì„¸ ê²°ê³¼ (my_experiments/kmmlu_ax_4.0_light_sft.json)
2. CSV: ê³¼ëª©ë³„ ë¹„êµ (my_experiments/kmmlu_sft_comparison.csv)

ì˜ˆìƒ ì‹œê°„: 30~40ë¶„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import os
import json
import torch
import pandas as pd
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datetime import datetime

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì„¤ì •
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# ëª¨ë¸ ê²½ë¡œ
BASE_MODEL = "skt/A.X-4.0-Light"  # ê¸°ë³¸ ëª¨ë¸
LORA_MODEL = "my_experiments/ax-kmmlu-sft"  # Fine-tuned LoRA

# ì¶œë ¥ íŒŒì¼
OUTPUT_JSON = "my_experiments/kmmlu_ax_4.0_light_sft.json"
OUTPUT_CSV = "my_experiments/kmmlu_sft_comparison.csv"
ZEROSHOT_JSON = "kmmlu_ax_4.0_light_zeroshot.json"  # ë¹„êµìš©

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 1. ëª¨ë¸ ë¡œë“œ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def load_model():
    """
    LoRA Fine-tuned ëª¨ë¸ ë¡œë“œ
    
    ê³¼ì •:
    1. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (A.X-4.0-Light)
    2. LoRA ì–´ëŒ‘í„° ì¶”ê°€
    3. GPUë¡œ ì´ë™
    
    ë¹„ìœ : êµê³¼ì„œ + ë³„ì±… ì¡°í•©
    """
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # ê¸°ë³¸ ëª¨ë¸ (êµê³¼ì„œ)
    print(f"1. ê¸°ë³¸ ëª¨ë¸: {BASE_MODEL}")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # LoRA ì–´ëŒ‘í„° (ë³„ì±…)
    print(f"2. LoRA ë¡œë“œ: {LORA_MODEL}")
    model = PeftModel.from_pretrained(base_model, LORA_MODEL)
    model = model.merge_and_unload()  # LoRAë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©
    
    # í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
    return model, tokenizer

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 2. í‰ê°€ í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def evaluate_kmmlu(model, tokenizer):
    """
    KMMLU ì „ì²´ í‰ê°€
    
    ê³¼ì •:
    1. 45ê°œ ê³¼ëª© ë¡œë“œ
    2. ê° ê³¼ëª©ë³„ ë¬¸ì œ í’€ê¸°
    3. ì •í™•ë„ ê³„ì‚°
    
    ë°˜í™˜: dict (ê³¼ëª©ë³„ ê²°ê³¼)
    """
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“Š KMMLU í‰ê°€ ì‹œì‘")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # KMMLU ë°ì´í„°ì…‹ 
    # 45ê°œ ê³¼ëª© ë¦¬ìŠ¤íŠ¸
    subjects = [
        'Accounting', 'Agricultural-Sciences', 'Aviation-Engineering-and-Maintenance',
        'Biology', 'Chemical-Engineering', 'Chemistry', 'Civil-Engineering',
        'Computer-Science', 'Construction', 'Criminal-Law', 'Ecology', 'Economics',
        'Education', 'Electrical-Engineering', 'Electronics-Engineering',
        'Energy-Management', 'Environmental-Science', 'Fashion', 'Food-Processing',
        'Gas-Technology-and-Engineering', 'Geomatics', 'Health', 'Industrial-Engineer',
        'Information-Technology', 'Interior-Architecture-and-Design', 'Law',
        'Machine-Design-and-Manufacturing', 'Management', 'Maritime-Engineering',
        'Marketing', 'Materials-Engineering', 'Mechanical-Engineering',
        'Nondestructive-Testing', 'Patent', 'Political-Science-and-Sociology',
        'Psychology', 'Public-Safety', 'Railway-and-Automotive-Engineering',
        'Real-Estate', 'Refrigerating-Machinery', 'Social-Welfare', 'Taxation',
        'Telecommunications-and-Wireless-Technology', 'Korean-History', 'Math'
    ]
    
    print(f"ê³¼ëª© ìˆ˜: {len(subjects)}ê°œ")
    print(f"ì˜ˆìƒ ì‹œê°„: 30~40ë¶„\n")
    
    results = {}
    
    # ê³¼ëª©ë³„ í‰ê°€
    for subject in tqdm(subjects, desc="ğŸ“ ê³¼ëª©ë³„ í‰ê°€"):
        # í•´ë‹¹ ê³¼ëª© ë¬¸ì œë§Œ í•„í„°
        dataset = load_dataset("HAERAE-HUB/KMMLU", subject)
        subject_data = dataset['test']
        
        correct = 0
        total = len(subject_data)
        
        for example in subject_data:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = format_prompt(example)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            prediction = get_prediction(model, tokenizer, prompt)
            
            # ì •ë‹µ ë¹„êµ
            if prediction == example['answer']:
                correct += 1
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = correct / total if total > 0 else 0
        results[subject] = {
            'correct': correct,
            'total': total,
            'accuracy': accuracy * 100
        }
    
    return results

def format_prompt(example):
    """
    KMMLU í”„ë¡¬í”„íŠ¸ í¬ë§·
    
    ë°ì´í„° êµ¬ì¡°:
    - question: ë¬¸ì œ
    - A, B, C, D: ì„ íƒì§€
    - answer: ì •ë‹µ (1, 2, 3, 4)
    """
    prompt = f"""ë‹¤ìŒ ë¬¸ì œë¥¼ í’€ê³  ì •ë‹µ ë²ˆí˜¸(1, 2, 3, 4)ë§Œ ë‹µí•˜ì„¸ìš”.

ë¬¸ì œ: {example['question']}
1. {example['A']}
2. {example['B']}
3. {example['C']}
4. {example['D']}


ì •ë‹µ:"""
    return prompt

def get_prediction(model, tokenizer, prompt):
    """
    ëª¨ë¸ ì˜ˆì¸¡
    
    ê³¼ì •:
    1. í”„ë¡¬í”„íŠ¸ í† í°í™”
    2. ëª¨ë¸ ìƒì„±
    3. ë‹µë³€ ì¶”ì¶œ (1, 2, 3, 4)
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=10,
            temperature=0.1,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    
    # ë‹µë³€ ì¶”ì¶œ (1, 2, 3, 4 ì¤‘ ì²« ë²ˆì§¸)
    for char in response:
        if char in ['1', '2', '3', '4']:
            return int(char)
    
    return 0  # ë‹µë³€ ì—†ìœ¼ë©´ 0 (ì˜¤ë‹µ ì²˜ë¦¬)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 3. ê²°ê³¼ ë¹„êµ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def compare_with_zeroshot(sft_results):
    """
    Zero-shot ê²°ê³¼ì™€ ë¹„êµ
    
    ì¶œë ¥: CSV (ê³¼ëª©ë³„ ë¹„êµí‘œ)
    """
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“Š Zero-shot ë¹„êµ")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # Zero-shot ê²°ê³¼ ë¡œë“œ
    if not os.path.exists(ZEROSHOT_JSON):
        print(f"âš ï¸  Zero-shot ê²°ê³¼ ì—†ìŒ: {ZEROSHOT_JSON}")
        return None
    
    with open(ZEROSHOT_JSON, 'r', encoding='utf-8') as f:
        zeroshot_results = json.load(f)
    
    # ë¹„êµ ë°ì´í„° ìƒì„±
    comparison = []
    
    for subject in sft_results:
        zs_acc = zeroshot_results.get(subject, {}).get('accuracy', 0)
        sft_acc = sft_results[subject]['accuracy']
        improvement = sft_acc - zs_acc
        
        comparison.append({
            'Subject': subject,
            'Zero-shot (%)': f"{zs_acc:.2f}",
            'SFT (%)': f"{sft_acc:.2f}",
            'Improvement (%p)': f"{improvement:+.2f}",
            'Correct (ZS)': zeroshot_results.get(subject, {}).get('correct', 0),
            'Correct (SFT)': sft_results[subject]['correct'],
            'Total': sft_results[subject]['total']
        })
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(comparison)
    
    # í‰ê·  ê³„ì‚°
    avg_zs = sum(float(row['Zero-shot (%)']) for row in comparison) / len(comparison)
    avg_sft = sum(float(row['SFT (%)']) for row in comparison) / len(comparison)
    avg_imp = avg_sft - avg_zs
    
    # í‰ê·  ì¶”ê°€
    df.loc[len(df)] = {
        'Subject': 'AVERAGE',
        'Zero-shot (%)': f"{avg_zs:.2f}",
        'SFT (%)': f"{avg_sft:.2f}",
        'Improvement (%p)': f"{avg_imp:+.2f}",
        'Correct (ZS)': '-',
        'Correct (SFT)': '-',
        'Total': '-'
    }
    
    # CSV ì €ì¥
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… ë¹„êµí‘œ ì €ì¥: {OUTPUT_CSV}\n")
    
    return df

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 4. ê²°ê³¼ ì¶œë ¥
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def print_summary(results, df=None):
    """
    ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    """
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ¯ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # ì „ì²´ í‰ê· 
    avg_acc = sum(r['accuracy'] for r in results.values()) / len(results)
    print(f"\nì „ì²´ í‰ê·  ì •í™•ë„: {avg_acc:.2f}%")
    
    # TOP 5 / BOTTOM 5
    sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
    
    print("\nğŸ”¥ TOP 5 (ê°€ì¥ ì˜í•œ ê³¼ëª©):")
    for i, (subject, data) in enumerate(sorted_results[:5], 1):
        print(f"  {i}. {subject:30s} {data['accuracy']:6.2f}%")
    
    print("\nâš ï¸  BOTTOM 5 (ì·¨ì•½ ê³¼ëª©):")
    for i, (subject, data) in enumerate(sorted_results[-5:], 1):
        print(f"  {i}. {subject:30s} {data['accuracy']:6.2f}%")
    
    # Zero-shot ë¹„êµ (ìˆìœ¼ë©´)
    if df is not None:
        avg_row = df[df['Subject'] == 'AVERAGE'].iloc[0]
        print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"ğŸ“ˆ Zero-shot ëŒ€ë¹„ í–¥ìƒ")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"Zero-shot: {avg_row['Zero-shot (%)']}")
        print(f"SFT:       {avg_row['SFT (%)']}")
        print(f"í–¥ìƒ:      {avg_row['Improvement (%p)']}")
        
        # ê°€ì¥ ë§ì´ ê°œì„ ëœ ê³¼ëª©
        df_sorted = df[df['Subject'] != 'AVERAGE'].sort_values(
            by='Improvement (%p)', 
            ascending=False
        )
        
        print(f"\nğŸš€ ê°€ì¥ ë§ì´ ê°œì„ ëœ ê³¼ëª© TOP 5:")
        for i, row in enumerate(df_sorted.head(5).itertuples(), 1):
            print(f"  {i}. {row.Subject:30s} {row._4}")
    
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# 5. ë©”ì¸ ì‹¤í–‰
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def main():
    print("="*60)
    print("ğŸ” A.X-4.0-Light SFT ëª¨ë¸ í‰ê°€")
    print("="*60)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ëª¨ë¸: {LORA_MODEL}")
    print(f"ì˜ˆìƒ ì‹œê°„: 30~40ë¶„")
    print("="*60 + "\n")
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    
    # 2. í‰ê°€
    results = evaluate_kmmlu(model, tokenizer)
    
    # 3. JSON ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {OUTPUT_JSON}")
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # 4. Zero-shot ë¹„êµ
    df = compare_with_zeroshot(results)
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print_summary(results, df)
    
    print(f"\nì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

if __name__ == "__main__":
    main()
